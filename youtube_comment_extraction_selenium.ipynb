{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "import pandas as pd \n",
    "from selenium.webdriver.common.by import By \n",
    "from selenium.webdriver.support.ui import WebDriverWait \n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_link = VIDEO_LINK = input()\n",
    "# yt_link = VIDEO_LINK = 'https://www.youtube.com/watch?v=x_VrgWTKkiM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comment_scraper(yt_link):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(yt_link)\n",
    "    \n",
    "    SCROLL_PAUSE_TIME = 5\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "\n",
    "    counter = 0\n",
    "    \n",
    "    while True:\n",
    "        # Scroll down to bottom\n",
    "        driver.execute_script(f\"window.scrollTo({last_height},document.documentElement.scrollHeight);\")\n",
    "\n",
    "        # Wait to load page\n",
    "        time.sleep(SCROLL_PAUSE_TIME)\n",
    "        \n",
    "        driver.execute_script(f\"scrollBy({last_height},-1000);\")\n",
    "        \n",
    "        \n",
    "        # Calculate new scroll height and compare with last scroll height\n",
    "        new_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        \n",
    "        if new_height == last_height:\n",
    "            print(\"break\")\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    \n",
    "    comment_div = driver.find_element_by_xpath('//*[@id=\"contents\"]')\n",
    "\n",
    "    buttons = driver.find_elements_by_xpath('//*[@id=\"more-replies\"]/a')\n",
    "\n",
    "    comments = comment_div.find_elements_by_xpath('//*[@id=\"content-text\"]')\n",
    "    \n",
    "    replies_elems =driver.find_elements_by_xpath('//*[@id=\"replies\"]')\n",
    "    \n",
    "    all_replies = [elem.text for elem in replies_elems]\n",
    "\n",
    "    name_elems = driver.find_elements_by_xpath('//*[@id=\"author-text\"]')\n",
    "\n",
    "    likes = comment_div.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')\n",
    "    \n",
    "    comments_scraped_wo_reps = {'comment_' + str(idx): (name.text, comment.text, likes.text)\\\n",
    "                                for idx, (name, comment, likes) in enumerate(zip(name_elems, comments, likes), 1)}\n",
    "    \n",
    "    \n",
    "    for button in buttons:\n",
    "        time.sleep(2)\n",
    "        webdriver.ActionChains(driver).move_to_element(button).click(button).perform()\n",
    "\n",
    "    replies = driver.find_elements_by_xpath('//*[@id=\"loaded-replies\"]/ytd-comment-renderer')\n",
    "    replies_scrapped = {f'reply_{idx}': reply.text for idx, reply in enumerate(replies, 1) if reply.text != ''}\n",
    "    comments = comment_div.find_elements_by_xpath('//*[@id=\"content-text\"]')\n",
    "    comments_txt = [comment.text for comment in comments]\n",
    "    name_elems = driver.find_elements_by_xpath('//*[@id=\"author-text\"]')\n",
    "    name_txt = [name.text for name in name_elems]\n",
    "    likes = driver.find_elements_by_xpath('//*[@id=\"vote-count-middle\"]')\n",
    "    likes_txt = [like.text for like in likes]\n",
    "    \n",
    "    \n",
    "    \n",
    "    driver.close()\n",
    "    \n",
    "    comments_scraped_with_reps = {'comment_' + str(idx): (name, comment, likes)\\\n",
    "                        for idx, (name, comment, likes) in enumerate(zip(name_txt, comments_txt, likes_txt), 1)}\n",
    "\n",
    "    return comments_scraped_with_reps, comments_scraped_wo_reps,all_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_info_scraper(yt_link):\n",
    "    \n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(yt_link)\n",
    "    \n",
    "    time.sleep(2)\n",
    "\n",
    "    driver.execute_script(\"window.scrollTo(0,document.documentElement.scrollHeight);\")\n",
    "\n",
    "    time.sleep(2)\n",
    "\n",
    "    driver.execute_script(\"scrollBy(0,-1000);\")\n",
    "\n",
    "    time.sleep(5)\n",
    "    \n",
    "    views = driver.find_element_by_xpath('//yt-view-count-renderer').text.split(' ')[0]\n",
    "    \n",
    "    title = driver.find_element_by_css_selector('.title').text\n",
    "\n",
    "    num_comments = float(driver.find_element_by_xpath('//ytd-comments-header-renderer').text.split(' ')[0])\n",
    "\n",
    "    num_likes_dislikes = driver.find_elements_by_xpath('//ytd-toggle-button-renderer')[0:2]\n",
    "\n",
    "    num_likes = num_likes_dislikes[0].text\n",
    "\n",
    "    num_dislikes = num_likes_dislikes[1].text\n",
    "    \n",
    "    scraping_incomplete = True\n",
    "    \n",
    "    driver.close()\n",
    "        \n",
    "    while scraping_incomplete:\n",
    "        \n",
    "        comments_scraped_with_reps, comments_scraped_wo_reps,all_replies = comment_scraper(yt_link)\n",
    "        \n",
    "        if float(len(comments_scraped_with_reps)) == num_comments:\n",
    "            \n",
    "            scraping_incomplete = False\n",
    "            \n",
    "    return {'video_title': title, 'num_views': views, 'num_comments': num_comments,\n",
    "            'num_likes': num_likes, 'num_dislikes': num_dislikes,\n",
    "            'comments_scraped_wo_reps': comments_scraped_wo_reps,\n",
    "            'comments_scraped_with_reps': comments_scraped_with_reps,\n",
    "           'all_replies':all_replies}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_values_to_list(comm):\n",
    "    for k,v in sorted(comm.items()):\n",
    "            comm[k]=list(comm[k])\n",
    "    return comm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_of_replies(all_replies,comments_scraped_wo_reps):\n",
    "    import re\n",
    "    for count in range(0,len(all_replies)):\n",
    "        if all_replies[count] is '':\n",
    "            all_replies[count]=0 \n",
    "            \n",
    "    for i in range(0,len(all_replies)):\n",
    "        num_replies=re.findall(r'\\d+',str(all_replies[i]))\n",
    "        if len(num_replies) is 0:\n",
    "            num_replies.append(1)\n",
    "        x=list(comments_scraped_with_reps)[i]\n",
    "        comments_scraped_wo_reps[x].append(num_replies[0])\n",
    "    \n",
    "    return comments_scraped_wo_reps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapping(cwor,cmr):\n",
    "    for k,v in (cwor.items()):\n",
    "        reply=v[3]\n",
    "        name=v[0]\n",
    "\n",
    "        for key,value in (cmr.items()):\n",
    "            if str(value[0]) == str(name):\n",
    "                next_key=key\n",
    "                key_index=list(cmr).index(next_key)\n",
    "            \n",
    "                break\n",
    "        \n",
    "        reply_com=[]       \n",
    "        for i in range(1,int(reply)+1):\n",
    "            next_ind=list(cmr)[key_index+i]\n",
    "            reply_com.append(cmr[next_ind])\n",
    "        \n",
    "        v.append(reply_com)\n",
    "        \n",
    "    return cwor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "break\n",
      "break\n"
     ]
    }
   ],
   "source": [
    "vedio_info = video_info_scraper(VIDEO_LINK)\n",
    "comments_scraped_with_reps, comments_scraped_wo_reps,all_replies=comment_scraper(VIDEO_LINK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_scraped_with_reps=convert_values_to_list(comments_scraped_with_reps)\n",
    "comments_scraped_wo_reps=convert_values_to_list(comments_scraped_wo_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_scraped_wo_reps=number_of_replies(all_replies,comments_scraped_wo_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_with_replies=mapping(comments_scraped_wo_reps,comments_scraped_with_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'comment_1': ['Victor zetterström',\n",
       "  'This series is really amazing! Love it!',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Awesome, thanks!', '1']]],\n",
       " 'comment_2': ['antonio rizzo',\n",
       "  'Bravo!!! So far the best explanation and support material on NN and DL',\n",
       "  '',\n",
       "  '0',\n",
       "  []],\n",
       " 'comment_3': ['Animesh Wakade',\n",
       "  \"Seymore I'm loving it.. FEED me the next ep!!\",\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"I'm a mean green mother from Western Europe and I'm fightin' mad! :)\",\n",
       "    '']]],\n",
       " 'comment_4': ['Patrick Jähne',\n",
       "  \"I love this series of explaination! It's easy to follow along. I can't wait for the next episode!\",\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Next one is last one :)', '']]],\n",
       " 'comment_5': ['Jousef Murad',\n",
       "  'Laurence is the best one in explaining these things! Laurence \"TheFeynman\" Moroney :D',\n",
       "  '2',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Haha! The Feynman! :)', '1']]],\n",
       " 'comment_6': ['Frederik Steinmetz',\n",
       "  \"I do have a follow-up question and I have spent days to figure it out. I used your example to train a model that can recognize pictures in a book. I works perfectly as long as I am in the python environment. But I can't seem to get it out of there. \\n I used model.save, but I have not found a method to convert that data into any format I can use in any other application. I am aiming for Tensorflow sharp, because I want to use it in Unity, but I also failed in tesorflowjs - where at least I got a comprehensible error message about the shape. Is this approach even meant to do anything outside this environment?\",\n",
       "  '1',\n",
       "  '0',\n",
       "  []],\n",
       " 'comment_7': ['Rohan Giriraj',\n",
       "  \"Hello Laurence! First off, thank you so much for these videos, they're really informative and helpful.\\nI tried implementing the code shown in this video, but I get this error:\\nValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (60000, 28, 28)\\nPlease do help. \\n\\nThanks!\",\n",
       "  '',\n",
       "  '2',\n",
       "  [['Laurence Moroney',\n",
       "    'Can you try it with my code (linked in description), and see if it goes away.',\n",
       "    '1'],\n",
       "   ['Felipe Maion',\n",
       "    '@Rohan Giriraj, the problem is that you need to reshape your training_images.\\n\\nJust add the following lines after loading the data:\\ntraining_images = training_images.reshape(60000, 28, 28, 1)\\ntest_images = test_images.reshape(10000, 28, 28, 1)\\n\\n\\nLet me know if this helps!',\n",
       "    '']]],\n",
       " 'comment_8': ['Algorunner',\n",
       "  'Another great video from Laurence! Thanks for the great lecture! Have a splendid day everyone. Good luck with your journey on conquering the world of Machine Learning!😁👍',\n",
       "  '',\n",
       "  '2',\n",
       "  [['Laurence Moroney',\n",
       "    \"Thanks! But instead of conquering the world, let's use our skills to make it a better place :)\",\n",
       "    ''],\n",
       "   ['Algorunner',\n",
       "    \"@Laurence Moroney Thank you so much for your reply Laurence! Just to clarify, I didn't mean conquering the physical world, I meant to conquer the world OF machine learning. As in, to reach a level of proficiency in machine learning skills! Have a fantastic day! Thank again, and yes, let's make this world a better place 😊👍\",\n",
       "    '']]],\n",
       " 'comment_9': ['Chowa C',\n",
       "  'Wishing the entire video series was uploaded at once .. waiting for the next video is torture 🤦',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Sorry!', '']]],\n",
       " 'comment_10': ['Warrior',\n",
       "  'Thanks for the video ♥️',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'My pleasure', '1']]],\n",
       " 'comment_11': ['Pa',\n",
       "  'best videos about ML i just love it its soo good even for me as an beginner. Thank you !',\n",
       "  '1',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Welcome! Glad you like :)', '']]],\n",
       " 'comment_12': ['Random Guy',\n",
       "  '4:05  why  64 does increasing this no. overfit the model',\n",
       "  '4',\n",
       "  '2',\n",
       "  [['Laurence Moroney',\n",
       "    \"It *might*. But more likely decreasing will lead to overfitting, increasing will lead to inefficiency of learning, as it's trying to learn more convolutions that match features when it may not need to. Think about it this way -- when distinguishing Rock/Paper/Scissors, how many unique 'items' would you need to have a set of rules to determine the differences between them? Maybe it's 64, but maybe only 20 would be enough etc. Part of the fun is in making your NNs as efficient as possible by removing stuff that while it might be useful, might also be not worth the training cost of having it in...\",\n",
       "    '7'],\n",
       "   ['Random Guy',\n",
       "    '@Laurence Moroney totally got you point. we should keep filters as high that it will not overfit and as low that it is cost efficient.',\n",
       "    '2']]],\n",
       " 'comment_13': ['Pn T',\n",
       "  'Sir can u make a project hand writing recognition use deep learning CNN',\n",
       "  '',\n",
       "  '0',\n",
       "  []],\n",
       " 'comment_14': ['JP Won',\n",
       "  'Thank you so much!!! from south korea',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Welcome!', '']]],\n",
       " 'comment_15': ['Ruben Guerra Chopite',\n",
       "  'Excelentes videos!!!! Son los mejores maestros que todos pueden desear! Thanks!!!!',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thank you! :)', '']]],\n",
       " 'comment_16': ['Peter MOUEZA',\n",
       "  'So well synthesized ! Thanks for your popularization !',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thanks!', '']]],\n",
       " 'comment_17': ['PAPA CHOUDHARY',\n",
       "  'Thanks!',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Welcome! :)', '']]],\n",
       " 'comment_18': ['balakrishna kumar',\n",
       "  \"I've been following the series and I must say it's an insightful video about CNN,could you also shed some light on backpropagation calculus.\",\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"That's a bit complex for this series, but I'll consider it for future vids\",\n",
       "    '1']]],\n",
       " 'comment_19': ['BHARATH NAGA CHANDRA Surampudi',\n",
       "  'where can i find part 1 and 2',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', \"They're on this channel...\", '']]],\n",
       " 'comment_20': ['Tony Thomas',\n",
       "  'Awesome! Suddenly everything makes sense. Thanks mate , you are the best',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Welcome!', '']]],\n",
       " 'comment_21': ['Peter Wilkinson',\n",
       "  'First again :D',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Beat me to it, again!', '']]],\n",
       " 'comment_22': ['Raja Sekhara Reddy Kaluri',\n",
       "  'Simple and awesome!!',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thanks, Raja', '']]],\n",
       " 'comment_23': ['Pushpajit Biswas',\n",
       "  'Sir How can I learn tf programming step by step from absolute beginning?',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"Try my Coursera 'Introduction to TensorFlow' course\",\n",
       "    '']]],\n",
       " 'comment_24': ['Rana Ahtsham',\n",
       "  \"It's hard to wait for every next video. Complete course playlist should be upload in a week. #\\nTensorFlow\",\n",
       "  '2',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"Was tempted to do that, but then the first one wouldn't be available until next week along with the last one :)\",\n",
       "    '1']]],\n",
       " 'comment_25': ['Gustavo J',\n",
       "  'Awesome!',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thanks!', '']]],\n",
       " 'comment_26': ['Mesh He',\n",
       "  '1:55 wrong',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'How?', '']]],\n",
       " 'comment_27': ['ashim karki',\n",
       "  'Can you guys make a video on \"how to insert custom dataset in tensorflow\"',\n",
       "  '',\n",
       "  '2',\n",
       "  [['Laurence Moroney',\n",
       "    \"You mean publish to TFDS? I'm working on one of those...\",\n",
       "    '1'],\n",
       "   ['Sinz Gaming Channel',\n",
       "    'whats problem in this?\\njust  create your custom data and load it though pandas.  simple',\n",
       "    '']]],\n",
       " 'comment_28': ['Rajan Saha Raju',\n",
       "  'Awesome',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thanks! :)', '']]],\n",
       " 'comment_29': ['Vamsi Belluri',\n",
       "  'Very nice video Laurence. You are a very teacher too 👍',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thank you! :)', '']]],\n",
       " 'comment_30': ['Maratimus Lion',\n",
       "  'Thank you , sir so much!!!',\n",
       "  '1',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Welcome!', '']]],\n",
       " 'comment_31': ['MrBoooniek',\n",
       "  'Fantastic!',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', ':)', '']]],\n",
       " 'comment_32': ['Anup Shete',\n",
       "  'This is really good for beginners, short and sweet videos... and alot to learn',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thanks!', '1']]],\n",
       " 'comment_33': ['fadop3zzz',\n",
       "  'Very nice tutorial!',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thanks!', '']]],\n",
       " 'comment_34': ['username',\n",
       "  'cool\\n!',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thanks!', '']]],\n",
       " 'comment_35': ['RAOUF KS',\n",
       "  'thank you GOOGLE',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Thank YOU!', '']]],\n",
       " 'comment_36': ['Aditya Chauhan',\n",
       "  'https://www.youtube.com/playlist?list=PLvf_ZGHboMr9DZ750qpW3G1dCKpXkDpvv',\n",
       "  '',\n",
       "  '0',\n",
       "  []],\n",
       " 'comment_37': ['Nitin Rai',\n",
       "  'Hehe i already visited github repo and tried all up coming parts of the video 😂 so smart of me... But these videos are really really very simple for anybody to get started for the first time',\n",
       "  '',\n",
       "  '3',\n",
       "  [['Laurence Moroney', \"That's cheating!\", '1'],\n",
       "   ['Raja Sekhara Reddy Kaluri',\n",
       "    '@Nitin Rai\\nCan you provide me link to GitHub repo??',\n",
       "    ''],\n",
       "   ['Nitin Rai',\n",
       "    '@Raja Sekhara Reddy Kaluri following along the Google colab notebook in the address link you can see the GitHub username\\nBtw i have forked the repo you can have a look https://github.com/imneonizer/mlday-tokyo',\n",
       "    '']]],\n",
       " 'comment_38': ['krishna muppalaneni',\n",
       "  'Need More Videos',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Me too!', '1']]],\n",
       " 'comment_39': ['balakrishna kumar',\n",
       "  \"I've got one query, if I have 32 filters in first CNN and 64 filters in 2nd CNN. When I pass an single image through first CNN block, then I will get 32 outputs from 32 filters . So does each output will go through those 64 filters in 2nd CNN block? \\nThat makes total outputs from 2nd CNN block will be 64x32= 2048 outputs by the end of 2nd CNN layer for one single image? \\n\\nHelp me to sort out this issue Mr.moroney.\\nThanks\",\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"That's a bit complex for this series, but I'll consider it for future vids\",\n",
       "    '1']]],\n",
       " 'comment_40': ['Maratimus Lion',\n",
       "  'But I prefer see what happens between layers on more simple example, it all about mess of coefs but anyway ...',\n",
       "  '1',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Welcome!', '']]],\n",
       " 'comment_41': ['Abhishiek Bhadauria',\n",
       "  'Can Anybody Please Tell Me What Is (3,3) In Conv 2D Code And Also Why The Input Shape Is (28,28,1) Not Just (28,28)?',\n",
       "  '1',\n",
       "  '2',\n",
       "  [['Felipe Maion',\n",
       "    '(28,28,1) means that it only has one layer... if it was color (28,28,3). As far as I know this is it. \\n(3,3) as far as I read in the docs is the strides (An integer or tuple/list of 2 integers, specifying the strides of the convolution along the height and width.)\\nCorrect me if I´m wrong.',\n",
       "    ''],\n",
       "   ['Laurence Moroney',\n",
       "    \"@Felipe Maion Yep -- exactly. If you look at 1:18, I'm using a 3x3 grid for the convolution. Hence (3,3) in the size. If I wanted 5x5 grids, for example, I'd change that parameter\",\n",
       "    '']]],\n",
       " 'comment_42': ['Javier Suárez',\n",
       "  'Hi Laurence, how do you choose in keras the filters (kernels)?, like the ones on your examples to filter vertical and horizontal lines?',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"Keras assigns filters pseudo-randomly, and then learns which ones 'work' over time. For the vertical/horizontal examples I hand-wrote code to apply them as a filter just to illustrate the point. Code is here:  https://github.com/lmoroney/dlaicourse/blob/master/Course%201%20-%20Part%206%20-%20Lesson%203%20-%20Notebook.ipynb\",\n",
       "    '1']]],\n",
       " 'comment_43': ['Psynames Synames',\n",
       "  'please create a playlist of only Introducing convolutional neural networks (ML Zero to Hero, parts) otherwise, i like your work. learning more from this.',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"It's part of the bigger 'Coding TensorFlow' show, and I prefer to keep it in that playlist, sorry!\",\n",
       "    '']]],\n",
       " 'comment_44': ['nesa1126',\n",
       "  'I had no idea that convolution and max puling were actually done like that (technically, or mathematically). I just imagined little sets of little neural networks. But I bet that it would be very heavy on processing resources.\\n\\n\\n(complete programing noob btw)',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney', 'Glad you liked! :)', '']]],\n",
       " 'comment_45': ['A Salam',\n",
       "  'Is tf.keras.layers the same as tf.layers?',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    'https://stackoverflow.com/questions/51089334/what-is-the-difference-between-tf-keras-layers-versus-tf-layers',\n",
       "    '1']]],\n",
       " 'comment_46': ['Dauda Sani Abdullahi',\n",
       "  'Laurence sir, am enjoying your series very well but am absolutely newbie to ML, i hope you recommend some beginner guide that i can put my hands on it for more understanding. Thanks, regards.',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    'This is a beginner guide :) -- To go higher, maybe look at Francois Chollet\\'s \"Deep Learning in Python\" book, at least the earlier chapters...',\n",
       "    '']]],\n",
       " 'comment_47': ['Frederik Steinmetz',\n",
       "  'How many thumbs up can I give?',\n",
       "  '',\n",
       "  1,\n",
       "  [['Rohan Giriraj',\n",
       "    \"Hello Laurence! First off, thank you so much for these videos, they're really informative and helpful.\\nI tried implementing the code shown in this video, but I get this error:\\nValueError: Error when checking input: expected conv2d_input to have 4 dimensions, but got array with shape (60000, 28, 28)\\nPlease do help. \\n\\nThanks!\",\n",
       "    '']]],\n",
       " 'comment_48': ['saad ali',\n",
       "  'i face one problem . i have 5 category of images i gather data set and ground truth and train cnn model and do prediction all well . but as soon as new category and when i run throught my trained model it categorize the new category into one of 5 category existing how . how can i prevent this false positive behaviour . the unseen image it predicts with high confidence which is a big problem',\n",
       "  '',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"If you have a new (6th) category, then you'll need a new model that can predict 6 categories, which you could get either by building a new one from scratch (recommended in this case) or through transfer learning.\",\n",
       "    '1']]],\n",
       " 'comment_49': ['Random Guy',\n",
       "  'You are awesome ❣️ from india',\n",
       "  '1',\n",
       "  1,\n",
       "  [['Laurence Moroney',\n",
       "    \"It *might*. But more likely decreasing will lead to overfitting, increasing will lead to inefficiency of learning, as it's trying to learn more convolutions that match features when it may not need to. Think about it this way -- when distinguishing Rock/Paper/Scissors, how many unique 'items' would you need to have a set of rules to determine the differences between them? Maybe it's 64, but maybe only 20 would be enough etc. Part of the fun is in making your NNs as efficient as possible by removing stuff that while it might be useful, might also be not worth the training cost of having it in...\",\n",
       "    '7']]],\n",
       " 'comment_50': ['ashim karki',\n",
       "  \"Why is there only one hidden layer ? Why don't use deep neural network!!!!!!\",\n",
       "  '',\n",
       "  '2',\n",
       "  [['Laurence Moroney',\n",
       "    \"You mean publish to TFDS? I'm working on one of those...\",\n",
       "    '1'],\n",
       "   ['Sinz Gaming Channel',\n",
       "    'whats problem in this?\\njust  create your custom data and load it though pandas.  simple',\n",
       "    '']]]}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_with_replies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "104"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(comments_scraped_with_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
